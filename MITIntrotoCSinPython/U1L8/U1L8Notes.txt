MIT Intro to CS with Python

Unit 1 Lesson 8
***************************

Efficiency is very important when doing real coding
The bigger the data, the more important efficiency
Basically: choose the right algorithm
Problem Reducing: reduce a problem to a previously solved problem

Efficiency: Space (memory) & Time

efficiency aka computational complexity

time to run influenced by
- speed of machine
- kind of python implimentation
- the input

Count # of basic steps
T: N -> N 
N = a natural number corresponding to input size
N = number of steps required for the input

A step is an operation that takes constant time

RAM = Random Access Machine (model of a computer)
actions executed one after another. sequential
constant time to access memory. each object takes = time to access

We use RAM to analyze algorithms

When thinking about how long an algorithm needs to run, consider
Best Case: minimum run time over all possible inputs  
Ex. Linear Search, desired object in first position.
Worst Case: maximum run time over all possible inputs
Ex. desired object not in sequence.
Expected/Average Case: usually we ignore this. Too many details, work. Not always enough information, assumptions required

Complexity analysis usually looks at the Worst Case. How bad can things get
Prepares us for the worst.

When calculating number of steps we ignore additive constants because the data sets we deal with are so big constants almost always irrel

We are interested in the relationship between growth in runtime and growth in size of input. Constants don't make a difference

Often we ignore multiplicative constants as well

Asymptotic growth interests us. How does step quantity grow as inputs reach their theoretical limit. How does function behave as n -> infin

Big Oh Notation. = time need
grows no faster than 
In ascending order of time required
O(1) = constant
O(log n) = log (usually base 2) 
O(n) = linear growth
O(n log n) = linear logarithmic
O(n ** c) = polynomial
O(c ** n) = exponential
what we usually mean is, worst case is about "O(blah)"

There's also a big theta notation for best case, grows no slower than

complexity must be expressed in terms of input

Upper bound for asymptotic growth of function. the limit
???memory heirarchy???

Always check if stuff inside the function might be more complex than expected.

RECITATION

Computer scientists like log base 2 because it's the bound time for bissection search

For algorithmic complexity we generally only consider the dominant term

Types of operation-time relations
mathematical = constant ie. number of operations
for loops = linear ie. number of times you loop
nested for loops can be polynomial
while loops can be polynomial

Big O tells us the number of steps, not secs/mins/hours

Limitations of big Oh
Ok, so O(1,000,000,000,000) is constant. So what? It's still too slow

This stuff only matters for limiting behavior, extremely large inputs
We think about inputs as they approach the limit of the possible size

Explicitly state what n equals.

Big Oh = worst case time, not amortized time

For finding the complexity of a recursive function
- Find the number of calls needed to reach the base case
- Find the complexity of the base case.

complexity of a recursive function of the form funk(n/b) is O(log bb n)
Bisection Search is O(log b2 n)

Loose bound on fibonacci complexity fib(n) is O(2**n)
More exact bound, O(Golden Ratio ** n)