MIT Intro to CS with Python

Unit 1 Lecture 9
***************************

The 'trick', the list is homogeneous. Every element is located at an easily predicatable location
Assume we have a list of ints, L
An integer occupies 4 units of memory
Location of L[i] = start + 4 * i
Each element of the list must be the same size to do this trick.

Linked List: Every element of a list is a pointer to the next element and (whose?) value.
To reach the ith element in a linked list O(i)

Indirection: A list is a list of pointers. Each element is of = size
With these kinds of lists we can use the 'trick'
that is, we know exactly where the ith element of a list is within that list. Location of ith element = start + i * size of elems
We can thus access any object in a list in constant time.
Basically, separation of pointers and value fields.


???Why use Linked Lists???
pointer and value may be very far apart.
???All problems can be solved by another level of indirection, except too many levels of indirection???

Binary Search only works on a sorted list

Binary Search O(log(len L)) on a sorted list.
Linear Search on an unsorted list is O(L)

We cannot sort a list in sub-linear time.
So sorting a list + Binary Search is slower than Linear Search 

???Why do Binary search at all???

Amortized Complexity: If we do lots of linear searches then we can sort it once too. Because sorting also uses a linear search. 

To do a sort we need to do a linear search. So if we need to do a ton of linear searches then sort might be worthwhile

Question: If we plan on k searches is
O(sort(L)) + k * log(len(L)) < k * len(L) ?
sort     k binary searches    k linear searches

So, for k searches: sort + k binary searches ? k linear searches

answer depends on both the number of searches, k, and length of list L
???does it depend on the length of list L???
The larger k, the more likely sorting is worthwhile

Many algorithms seek to establish and maintain an invariant
for selection sort:
Pointer into list divides list into prefix and suffix.
The invariant = prefix is always sorted
We start with empty prefix. Each step increases prefix by 1 el.

Why does

A = ['cat', 'dog']
B = A
B[1] = 'ninja'
print A
#prints ['cat', 'ninja']
# Here when we set B = A we did not use a copy of A, we used the objects A was attached to
# We have aliased A, B, and ['cat', 'dog'] that object
# Is it that we use copies of immutable objects but the objects themselves for mutables
L = ['cat', 'dog']
temp = L[0]
L[0] = L[1]
L[1] = temp
print L[0], L[1], temp, L
# Clearly we are copying values. temp = the value pointer at index 0 points at

The O stands for Order. Like a first "order" polynomial

Merge Sort: Divide and Conquer
Has three steps
1.) Pick a threshold input size at which we stop dividing and solve
2.) How many divisions are produced at each division
3.) Algorithm for combining sub-solutions

Step 3
You can merge two sorted lists rather quickly.

- compare first element to first element
- make smaller the first element of the merged list.
- then compare the next element of the list which contained the smaller element with the first element of the other list. Make the smaller element the second element of the merged list. Keep going.

n copies where n = len(first_list) + len(second_list)
n comparisons where n = len(longer_list) BULLSHIT 
ie. if largest ele is the last el in short list OR short list contains the second largest el BULLSHIT
n comparisons where n = len(first) + len(second) - 1
So, each merge is O(2n - 1) or O(n)

We call merge the number of times we do sort
We do sort log n times

We're gonna break list into lists of length 1, obviously sorted.
Then we merge lists of 1. Then we merge lists of 2, etc.
For a list of len(x) we must do, where n = len(x)
2 ** 0 + 2 ** 1 + ... + 2 ** log(n-1) or n-1 merges

We call merge log b2 n times

So these two steps produce O(n * log n)

???functional arguments???

float.__alreadydefinedfunction__
the sample code works fine without it...

We keep branching in two until we arrive at elements of length 1
Obviously a list of length 1 is ordered
Then we start merg-sorting each branching which we can do easily since the lists are ordered.

Merge sort is O(n * log(n))

We use an ordering function that returns booleans as an argument in our sort function 
